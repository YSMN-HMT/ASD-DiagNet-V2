#*********************************************************
# All of the used functions are defined here, but Train and Test
#*********************************************************

def get_label(file_name):
    assert (file_name in labels)
    return labels[file_name]

#**********************************************************	
def get_corr_data(filename):
    #print(filename)
    for file in os.listdir(data_main_path):
        if file.startswith(filename):
          df = pd.read_csv(os.path.join(data_main_path, file), sep='\t')
    
    with np.errstate(invalid="ignore"):
        corr = np.nan_to_num(np.corrcoef(df.T))
        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))
        m = ma.masked_where(mask == 1, mask)
        return ma.masked_where(m, corr).compressed()

#**********************************************************		
def get_corr_matrix(filename):
    for file in os.listdir(data_main_path):
        if file.startswith(filename):
            df = pd.read_csv(os.path.join(data_main_path, file), sep='\t')
    with np.errstate(invalid="ignore"):
        corr = np.nan_to_num(np.corrcoef(df.T))
        return corr

#**********************************************************
def confusion(g_turth,predictions):

    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()
    accuracy = (tp+tn)/(tp+fp+tn+fn)
    sensitivity = (tp)/(tp+fn)
    specificty = (tn)/(tn+fp)    
    return accuracy,sensitivity,specificty

#**********************************************************
def get_regs(samplesnames,regnum):
    datas = []
    for sn in samplesnames:
      datas.append(all_corr[sn][0])
    datas = np.array(datas)
    avg=[]
    for ie in range(datas.shape[1]):
      avg.append(np.mean(datas[:,ie]))
    avg=np.array(avg)
    highs=avg.argsort()[-regnum:][::-1] 
    lows=avg.argsort()[:regnum][::-1] 
    regions=np.concatenate((highs,lows),axis=0)
    return regions
	
#**********************************************************
def norm_weights(sub_flist):
	num_dim = len(eig_data[flist[0]]['eigvals'])
	norm_weights = np.zeros(shape=num_dim)
	for f in sub_flist:
      norm_weights += eig_data[f]['norm-eigvals'] 
	return norm_weights

#**********************************************************
def cal_similarity(d1, d2, weights, lim=None):# EROS
	res = 0.0
	if lim is None:
      weights_arr = weights.copy()
	else:
      weights_arr = weights[:lim].copy()
      weights_arr /= np.sum(weights_arr)
	for i,w in enumerate(weights_arr): 
      res += w*np.inner(d1[i], d2[i]) 
	return res

#**********************************************************

if not os.path.exists('File directory for .pkl file'):
    all_corr = {}
    for f in flist:
        lab = get_label(f)
        all_corr[f] = (get_corr_data(f), lab)

    print('Corr-computations finished')

    pickle.dump(all_corr, open('File directory for .pkl file', 'wb'))
    print('Saving to file finished')

else:
    all_corr = pickle.load(open('File directory for .pkl file', 'rb'))
    print("correlation added")
