#**********************************************************
#Design your AutoEncoder by Adding Sequential layers respectively.
#If this is tied, No need to design AutoDecoder seprately. Just add your classifier as the bottleneck.
#**********************************************************

import keras
class MTAutoEncoder(tf.keras.Model):
  def __init__(self, num_inputs=990,
                num_latent=200, tied=True,
                num_classes=2, use_dropout=False):
      super(MTAutoEncoder, self).__init__()
      self.tied = tied
      self.num_inputs = num_inputs
      self.use_dropout = use_dropout
      self.num_latent = num_latent
      self.fc_encoder1 =layers.Dense(1000,input_shape = (9950,), activation= 'tanh')

      self.fc_encoder2=layers.Dense(500, activation= 'tanh')
      self.fc_encoderFinal = tf.keras.layers.Dense(100, activation= 'tanh') 

      if not tied:
          self.fc_decoder = tf.keras.layers.Dense(num_inputs,activation= 'tanh' )

      self.classifier1 = keras.Sequential([                                                                               
                                          
                                          layers.Dense(50, activation='elu'),
                                          layers.Dense(10, activation='elu')
										  ])
      
   
      self.drp =tf.keras.layers.Dropout(0.5)
      self.btch = tf.keras.layers.BatchNormalization()
      self.finalclf= tf.keras.layers.Dense(1) 
	  
	  
  def call(self, x, eval_classifier = False , training = True):

      x = self.btch(x,training=training)
      x= self.fc_encoder1(x)
      x= self.fc_encoder2(x)
      x = self.fc_encoderFinal(x)
      
      if eval_classifier:
        x_logit = self.classifier1(x)
        x_logit = self.drp(x_logit,training=training)
        x_logit = self.finalclf(x_logit)
      else:
        x_logit = None

      if self.tied: 
        # pdb.set_trace()
        y = tf.matmul(x,self.fc_encoderFinal.weights[0], transpose_b= True)
        y = tf.matmul(y,self.fc_encoder2.weights[0], transpose_b= True)
        y = tf.matmul(y,self.fc_encoder1.weights[0], transpose_b= True)
      else:
        y = self.fc_decoder(x)          
      return y, x_logit

mtae = MTAutoEncoder()
#Save your model if you want: mtae.save_weights('mtae.h5')